{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bf09d98c-446e-4e4d-bec0-cfa4dac793db",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from pathlib import Path\n",
    "import requests\n",
    "import json\n",
    "import pandas as pd\n",
    "import os \n",
    "import re \n",
    "\n",
    "import faiss\n",
    "\n",
    "import os\n",
    "import faiss\n",
    "import json\n",
    "import numpy as np\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "from typing import List"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ec2df972-e077-463c-b111-4bd5cb669d61",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! pip install pandas\n",
    "# ! pip install faiss-cpu\n",
    "# ! pip install sentence_transformers\n",
    "# ! pip install einops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e9fac57b-f63c-4a54-afa3-f501b0392a06",
   "metadata": {},
   "outputs": [],
   "source": [
    "CHUNK_SIZE = 512\n",
    "CHUNK_OVERLAP = 64\n",
    "\n",
    "def chunk_text(text, size=CHUNK_SIZE, overlap=CHUNK_OVERLAP):\n",
    "    words = text.split()\n",
    "    chunks = []\n",
    "    for i in range(0, len(words), size - overlap):\n",
    "        chunks.append(\" \".join(words[i:i+size]))\n",
    "    return chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9f6482f2-1059-4134-8ead-9c75a44b1990",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<All keys matched successfully>\n"
     ]
    }
   ],
   "source": [
    "# Setup paths\n",
    "faiss_base_path = \"./faiss-index-v1\"\n",
    "index_path = os.path.join(faiss_base_path, \"faiss_index.index\")\n",
    "metadata_path = os.path.join(faiss_base_path, \"faiss_metadata.json\")\n",
    "\n",
    "# Load model\n",
    "model = SentenceTransformer(\"nomic-ai/nomic-embed-text-v1.5\", trust_remote_code=True)  \n",
    "\n",
    "index = faiss.IndexFlatL2(768) \n",
    "metadata_store = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7f7a27f7-fcaf-4008-8fd0-c9467182f24c",
   "metadata": {},
   "outputs": [],
   "source": [
    "doctype = \"union-act\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0809dffc-60d2-4299-a7f0-a62c72443ba7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Working on file 1044"
     ]
    }
   ],
   "source": [
    "%timeit\n",
    "doctype_path_processed_metdata = os.path.join(\"documents-v1\", \"metadata\")\n",
    "doctype_path_processed_content = os.path.join(\"documents-v1\", \"content\")\n",
    "\n",
    "txt_files = [f for f in os.listdir(doctype_path_processed_content) if f.endswith('.txt')]\n",
    "\n",
    "count = 0 \n",
    "for t_file in txt_files: \n",
    "    \n",
    "    print(f\"Working on file {count}\", end='\\r')\n",
    "    f_path_processed_metadata = os.path.join(doctype_path_processed_metdata, t_file.replace('.txt', '.json'))\n",
    "    f_path_processed_content = os.path.join(doctype_path_processed_content, t_file)\n",
    "    \n",
    "    with open(f_path_processed_content, 'r', encoding='utf-8') as file:\n",
    "        processed_text = file.read()\n",
    "        \n",
    "    with open(f_path_processed_metadata, 'r', encoding='utf-8') as f:\n",
    "        metadata = json.load(f)\n",
    "            \n",
    "    chunks = chunk_text(processed_text)\n",
    "\n",
    "    embeddings = model.encode(chunks)\n",
    "    index.add(np.array(embeddings))\n",
    "    \n",
    "    for i, chunk in enumerate(chunks):\n",
    "        metadata_store.append({\n",
    "            \"source_file\": t_file,\n",
    "            \"chunk_index\": i,\n",
    "            \"text\": chunk, \n",
    "            \"doctype\": doctype, \n",
    "            \"title\": metadata['title']\n",
    "        })\n",
    "        \n",
    "    count += 1\n",
    "    if count % 50 == 0: \n",
    "        faiss.write_index(index, index_path)\n",
    "        with open(metadata_path, 'w') as f:\n",
    "            json.dump(metadata_store, f, indent=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c14b41b8-472f-451b-9e74-272c7fe932fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len(metadata) corresponding to 1045 files: 29181\n"
     ]
    }
   ],
   "source": [
    "print(f\"len(metadata) corresponding to {count} files: {len(metadata_store)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4004e85a-278f-4ecb-bbd8-bfa52d22bd67",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
